{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pro Tips\n",
    "- Some concepts that are important in data analysis and common in the field, include overfitting, regression towards the mean, curse of dimensionality, importance of visualization, and inductive bias. These questions test your knowledge and experience with some of the hazards of blind data analysis and your ability to distinguish a significant result from a spurious one.\n",
    "- Consider asking your interviewer how data scientists extract and wrangle data at the company, what tools the team uses to do its exploratory analysis, and how the company shares its findings internally. Most of the work is not the analysis. In fact, data scientists spend most of their time just getting, cleaning, and processing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (Given a Dataset) Analyze this dataset and tell me what you can learn from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kaggle](https://www.kaggle.com/)\n",
    "\n",
    "**Pro Tip**<br>\n",
    "- If asked to analyze a dataset during the interview, the interviewer is looking to learn about your comfort with your statistical software and your ability to generate interesting insights in a short period of time. We recommend making visualizations first, to show that you know good practices, prevent future missteps, and identify possible transformation needed. Be sure to talk about your procedure and anticipate questions about your approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is $ R^2 $? What are some other metrics that could be better than $ R^2 $ and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R<sup>2</sup> measures how close the data points are to the fitted line. It is essentially the proportion of the variance explained by the fitted line.\n",
    "\n",
    "$$ R^2 = \\frac{TSS - RSS}{TSS}$$ \n",
    "$$ RSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\ \\ and \\ \\ TSS = \\sum_{i=1}^{n} (y_i - \\bar{y}_i)^2 $$, where RSS is the residual sum of squares and TSS is the total sum of squares.\n",
    "\n",
    "A low R<sup>2</sup> value doesn't always mean the line is a bad fit and a high R<sup>2</sup> value doesn't always mean the line is a good fit. You should evaluate R-squared values in conjunction with residual plots, other model statistics, and subject area knowledge.[<sup>1](http://blog.minitab.com/blog/adventures-in-statistics/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit)\n",
    "\n",
    "The problem with using the R<sup>2</sup> metric is that every time you add a predictor to a model, the R-squared increases, even if due to chance alone. A better metric to address this problem is the adjusted R<sup>2</sup> value. The adjusted R<sup>2</sup> value is a modified version of the R<sup>2</sup> value that adjusts for the number of predictors in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What is the curse of dimensionality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curse of dimensionality is the phenomena where adding dimensions to space increases the volume of the space so rapidly that the data becomes sparse very quickly and data is needed at an exponential rate in relation to the number of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Is more data always better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, if your data is biased or irrelevant, getting more of it won't help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. What are advantages of plotting your data before performing analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting your data before performing analysis can help with data understanding before analysis. Plotting can show if your data is exhibiting any strange patterns, if there are duplicates or missing data, or if there are any outliers. It can also help you understand which model might fit your data best before you even build the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. How can you make sure that you don’t analyze something that ends up meaningless?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the [CRISP-DM](https://en.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining) framework.\n",
    "1. Business Understanding\n",
    "2. Data Understanding\n",
    "3. Data Preparation\n",
    "4. Modeling\n",
    "5. Evaluation\n",
    "6. Deployment\n",
    "\n",
    "It is important to have a clear business question at the very beginning of the process and to always refer back to the business question every step of the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. What is the role of trial and error in data analysis? What is the the role of making a hypothesis before diving in?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only reason I can think of to use trial and error in data analysis is if you're hypertuning a parameter. The logical way to do this would be to use grid search, but perhaps you have a lot of time on your hands, or you're using trial and error to refine your grid search.\n",
    "\n",
    "Making a hypothesis before diving in, in any scientific experiment, not just in data analysis, allows for a structure to be followed for the experiment/analysis. When you establish a hypothesis, you have something concrete to disprove or fail to disprove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. How can you determine which features are the most important in your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run different feature importance algorithms and plots for tree ensemble methods to determine which features are having how much of an effect on your predictive model.\n",
    "\n",
    "Also for tree ensemble algorithms, you can use Entropy or Gini Impurity equations and choose the best information gain to determine where the best split at each step of the tree is.<br>\n",
    "**Entropy** $$ H(S) = -\\sum_{i=1}^m P(c_i) log_2(P(c_i)) $$\n",
    "\n",
    "**Gini Impurity** $$ H(S) = \\sum_{i=1}^m P(c_i)(1 - P(c_i)) $$\n",
    "\n",
    "**Information Gain** $$ Gain(S,D) = H(S) - \\sum_{V \\in D} \\frac {|V|}{|S|} H(V)$$\n",
    "where S is the original (parent) set, D is the split, V is the subset (child) of S.\n",
    "\n",
    "See number [12](#12) for notes on feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. How do you deal with some of your predictors being missing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Delete all cases with a missing value.** This can decrease the power of the analysis by decreasing the sample size.\n",
    "- **Replace any missing value with the mean.** This has the benefit of not changing the sample mean for that variable. \n",
    "- **Build another predictive model to predict the missing values.** This could be a whole project in itself, so simple techniques are usually used here.\n",
    "- **Use a model that can incorporate missing data.** Like a random forest, or any tree-based method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. You have several variables that are positively correlated with your response, and you think combining all of the variables could give you a good prediction of your response. However, you see that in the multiple linear regression, one of the weights on the predictors is negative. What could be the issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Multicollinearity**](https://en.wikipedia.org/wiki/Multicollinearity) is the phenomenon in which two or more predictor variables in a multiple regression model are highly correlated, meaning that one can be linearly predicted from the others with a substantial degree of accuracy.\n",
    "\n",
    "A principal danger of such data redundancy is that of overfitting in regression analysis models. The best regression models are those in which the predictor variables each correlate highly with the dependent (outcome) variable but correlate at most only minimally with each other.\n",
    "\n",
    "Not part of the question, but in case you want concrete answers about how to deal with multicollinearity: [click here](https://en.wikipedia.org/wiki/Multicollinearity#Remedies_for_multicollinearity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Let’s say you’re given an unfeasible amount of predictors in a predictive modeling task. What are some ways to make the prediction more feasible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dimension reduction**: Use [principal component analysis (PCA)](http://setosa.io/ev/principal-component-analysis/) to map high dimensional spaces onto smaller dimensional spaces by finding linear combinations of predictors that maximize the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='12'></a>\n",
    "### 12. Now you have a feasible amount of predictors, but you’re fairly sure that you don’t need all of them. How would you perform feature selection on the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning community classifies feature selection into 3 different categories:[<sup>1](https://www.quora.com/How-do-I-perform-feature-selection/answer/Vincent-Firmansyah?srid=fkun)\n",
    "\n",
    "- **Filter methods**: These include simple statistical test to determine if a feature is statistically significant.\n",
    "    - For example, the *p value* for a t test to determine if the null hypothesis should be accepted and the feature rejected. \n",
    "    - Does not take into account feature interactions and is generally not a very recommended way of doing feature selection as it can lead to loss in information.\n",
    "\n",
    "- **Wrapper based methods**: This involves using a learning algorithm to report the optimal subset of features. \n",
    "    - For example, use tree based models to determine the importance of features by looking at the information gain. \n",
    "    - This can give a brief overview of which features are important to help provide some informal validation of engineered features. \n",
    "    - Tree based models are also robust against issues like multi-collinearity, missing values, outliers, etc. \n",
    "    - This can also allow you to discover some interactions between features. \n",
    "    - However, this can be rather computationally expensive.\n",
    "\n",
    "- **Embedded Methods**: This involves carrying out feature selection and model tuning at the same time. \n",
    "    - For example, the [Lasso](https://en.wikipedia.org/wiki/Lasso_(statistics) method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces.\n",
    "    \n",
    "In class, we used the terms [metrics](https://github.com/zipfian/DSI_Lectures/blob/master/regularized-regression/notes/notes.md#metrics) for filter methods, [subset selection](https://github.com/zipfian/DSI_Lectures/blob/master/regularized-regression/notes/notes.md#subset-selection) for wrapper based methods, and [shrinkage methods](https://github.com/zipfian/DSI_Lectures/blob/master/regularized-regression/notes/notes.md#shrinkage-methods) for embedded methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Your linear regression didn't run and communicates that there are an infinite number of best estimates for the regression coefficients. What could be wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Data leakage](http://insidebigdata.com/2014/11/26/ask-data-scientist-data-leakage/) is when the data you are using to train a machine learning algorithm happens to have the information you are trying to predict.\n",
    "\n",
    "Data leakage can manifest in many ways including:\n",
    "- Leaking data from the test set into the training set.\n",
    "- Leaking the correct prediction or ground truth into the test data.\n",
    "- Leaking of information from the future into the past.\n",
    "- Reversing obfuscation, randomization or anonymization of data that were intentionally included.\n",
    "- Information from data samples outside of scope of the algorithm’s intended use.\n",
    "- Any of the above existing in external data coupled with the training set.\n",
    "\n",
    "For example, consider the use of a “customer service rep name” feature variable in a SaaS company churn prediction algorithm. Using the name of the rep who interviewed a customer when they churned might seem innocent enough until you find out that a specific rep was assigned to take over customer accounts where customers had already indicated they intended to churn. In this case, the resulting algorithm would be highly predictive of whether the customer had churned but would be useless for making predictions on new customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. You run your regression on different subsets of your data, and find that in each subset, the beta value for a certain variable varies wildly. What could be the issue here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Heteroskedasticity](https://en.wikipedia.org/wiki/Heteroscedasticity) is the circumstance in which the variability of a variable is unequal across the range of values of a second variable that predicts it.[<sup>1](http://www.statsmakemecry.com/smmctheblog/confusing-stats-terms-explained-heteroscedasticity-heteroske.html)\n",
    "\n",
    "There are four common corrections for heteroscedasticity. They are:\n",
    "- View logarithmized data.\n",
    "- Use a different specification for the model.\n",
    "- Apply a [weighted least squares](https://en.wikipedia.org/wiki/Least_squares#Weighted_least_squares) estimation method.\n",
    "- [Heteroscedasticity-consistent standard errors (HCSE)](https://en.wikipedia.org/wiki/Heteroscedasticity-consistent_standard_errors). This method corrects for heteroscedasticity without altering the values of the coefficients. This method may be superior to regular OLS because if heteroscedasticity is present it corrects for it, however, if the data is homoscedastic, the standard errors are equivalent to conventional standard errors estimated by OLS. \n",
    "\n",
    "Heteroscedasticity often occurs when there is a large difference among the sizes of the observations.\n",
    "- A classic example of heteroscedasticity is that of income versus expenditure on meals. As one's income increases, the variability of food consumption will increase. A poorer person will spend a rather constant amount by always eating inexpensive food; a wealthier person may occasionally buy inexpensive food and at other times eat expensive meals. Those with higher incomes display a greater variability of food consumption.\n",
    "- Another example is to imagine you are watching a rocket take off nearby and measuring the distance it has traveled once each second. In the first couple of seconds your measurements may be accurate to the nearest centimeter, say. However, 5 minutes later as the rocket recedes into space, the accuracy of your measurements may only be good to 100 m, because of the increased distance, atmospheric distortion and a variety of other factors. The data you collect would exhibit heteroscedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. What is the main idea behind ensemble learning? If I had many different models that predicted the same response variable, what might I want to do to incorporate all of the models? Would you expect this to perform better than an individual model or worse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea behind ensemble learning is to combine multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.[<sup>1](https://en.wikipedia.org/wiki/Ensemble_learning)\n",
    "\n",
    "To incorporate many different models that predicted the same response variable, you can use the outputs for all of the models as inputs for a meta-model.[<sup>2](https://www.quora.com/What-are-the-best-methods-for-combining-different-machine-learning-models-to-get-a-better-prediction-than-any-individual-model/answers/6785936?srid=fkun)\n",
    "\n",
    "You should expect an ensemble method to do better than an individual model for the following reasons:[<sup>3](https://www.quora.com/How-do-ensemble-methods-work-and-why-are-they-superior-to-individual-models/answer/William-Chen-6?srid=fkun)\n",
    "- **They average out biases.** For example, if you average a bunch of democratic-leaning polls and a bunch of republican-leaning polls together, you will get on average something that isn't leaning either way\n",
    "- **They reduce the variance.** The aggregate opinion of a bunch of models is less noisy than the single opinion of one of the models. For example, in finance, this is called diversification - a mixed portfolio of many stocks will be much less variable than just one of the stocks alone. This is also why your models will be better with more data points rather than fewer.\n",
    "- **They're unlikely to overfit.** If you have individual models that didn't overfit, and you're combining the predictions from each model in a simple way (average, weighted average, or logistic regression), then there's no room for overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Given that you have wifi data in your office, how would you determine which rooms and areas are underutilized and overutilized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. How could you use GPS data from a car to determine the quality of a driver?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. Given accelerometer, altitude, and fuel usage data from a car, how would you determine the optimum acceleration pattern to drive over hills?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. Given position data of NBA players in a season’s games, how would you evaluate a basketball player’s defensive ability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. How would you quantify the influence of a Twitter user?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21. Given location data of golf balls in games, how would you construct a model that can advise golfers where to aim?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22. You have 100 mathletes and 100 math problems. Each mathlete gets to choose 10 problems to solve. Given data on who got what problem correct, how would you rank the problems in terms of difficulty?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23. You have 5000 people that rank 10 sushis in terms of saltiness. How would you aggregate this data to estimate the true saltiness rank in each sushi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24. Given data on congressional bills and which congressional representatives co-sponsored the bills, how would you determine which other representatives are most similar to yours in voting behavior? How would you evaluate who is the most liberal? Most republican? Most bipartisan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25. How would you come up with an algorithm to detect plagiarism in online content?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26. You have data on all purchases of customers at a grocery store. Describe to me how you would program an algorithm that would cluster the customers into groups. How would you determine the appropriate number of clusters to include?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 27. Let’s say you’re building the recommended music engine at Spotify to recommend people music based on past listening history. How would you approach this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
