{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pro Tips\n",
    "\n",
    "- Proper A/B testing practices are often a common discussion, especially because it easily becomes  more complicated than anticipated in practice. Multiple variants and metrics, simultaneous conflicting experiments, and improper randomization will complicate experiments. Most people do not have a formal academic background on experimental design.\n",
    "- Important concepts to know include [randomization](#Randomization), [Simpson's paradox](#simpsons_paradox), and [multiple comparisons](#multiple_comparisons). Advanced concepts to know that may impress interviewers includes alternatives to A/B testing (such as multi-armed bandit strategies), or alternatives to t-tests and z-tests (e.g. non-parametric methods, boot-strapping)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Randomization'></a>\n",
    "### 1. In an A/B test, how can you check if assignment to the various buckets was truly random?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no statistical test for randomness, but the more important attribute of an experiment is whether or not the buckets are balanced. In a truly randomized A/B test, the treatment and control groups would have equivalent distributions of variables and similar outcomes prior to the treatment being administered.\n",
    "\n",
    "To test your experiment:\n",
    "- You can examine moments of distribution like mean or standard deviation.\n",
    "- You can compare distributions with the Kolmogorov-Smirnov test.\n",
    "- You can train a logit model to predict if an observation is in treatment or control group.\n",
    "\n",
    "These are only ways that you can test your experiment. You can only either reject the null hypothesis or fail to reject the null hypothesis.\n",
    "\n",
    "The best way to perform a randomized A/B test is to conduct the experiment several times with resampling. Be sure to set a signifcance level and compare the confidence intervals for each iteration of the experiment. If you do not have the resources necessary for this, consider running an A/A test before running an A/B test. See number [2](#2) for notes on A/A testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "### 2. What might be the benefits of running an A/A test, where you have two buckets who are exposed to the exact same product?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An A/A test helps you to test your test. \n",
    "\n",
    "If you are seeing statistically significant differences between A and A, you may have the following problems:\n",
    "- The two buckets actually are being exposed to different products/pages.\n",
    "- The buckets are not randomized properly - each user does not in fact have an equal chance of being in either group (e.g., maybe your randomizer is more likely to put certain users in the first group).\n",
    "- Your hypothesis test is biased and your data may violate the assumptions of the test that you are using (e.g. using a t-test when you have heavily influential outliers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What would be the hazards of letting users sneak a peek at the other bucket in an A/B test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letting users sneak a peek at the other bucket in an A/B test might influence how they react to the test. You are essentially introducting a new variable to the test and you are testing not only the treatment, but also the change in thought processes of users.\n",
    "\n",
    "A good example of why sneak peeks are bad:<br>\n",
    "Suppose you wanted to test how your audience reacts to a promotion. You set up three versions: the control; 15% off; and 25% off. Users who were in the 15% bucket but saw the 25% discount might change their behavior. For instance, they might go from feeling “Great! I’m getting a good deal” to “Why am I being cheated by only being given a 15% discount and not a 25% discount?” This group might become less likely to convert and so artificially depress your results. You might see the opposite effect for the 25% group.[<sup>1](https://www.quora.com/What-would-be-the-hazards-of-letting-users-sneak-a-peek-at-the-other-bucket-in-an-A-B-test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What would be some issues if blogs decide to cover one of your experimental groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the scenario that the blog is covering one of your experimental groups while your experiment is ongoing, you would have the same situation as stated in number [3](#3) where users are getting a sneak peak at, not only a bucket in the A/B test, but also the progress or outcome of the experimental group.\n",
    "\n",
    "In the scenario that the blog is covering one of your experimental groups after your experiment is concluded, you would lose the opportunity to replicate the experiment for the same reasons as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. How would you conduct an A/B test on an opt-in feature? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Randomly split sample of email list into two groups - treatment and control (\"A\" and \"B\" groups).\n",
    "- Send two types of email with two variations, say “A” and “B”. Then track the response rate (on both “A” and “B” email type) based on rate of open, views, clicks and finally sales conversion.\n",
    "- Calculate statistical significance of the two email groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. How would you run an A/B test for many variants, say 20 or more?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Analysis of variance (ANOVA)**](https://en.wikipedia.org/wiki/Analysis_of_variance): In its simplest form, ANOVA provides a statistical test of whether or not the means of several groups are equal, and therefore generalizes the t-test to more than two groups. ANOVAs are useful for comparing (testing) three or more means (groups or variables) for statistical significance. It is conceptually similar to multiple two-sample t-tests, but is more conservative (results in less type I error) and is therefore suited to a wide range of practical problems.\n",
    "- [**Fixed effects models**](https://en.wikipedia.org/wiki/Fixed_effects_model) apply to situations in which the experimenter applies one or more treatments to the subjects of the experiment to see whether the response variable values change. This allows the experimenter to estimate the ranges of response variable values that the treatment would generate in the population as a whole. They represent the observed quantities in terms of explanatory variables that are treated as if the quantities were non-random.\n",
    "- [**Random effects models**](https://en.wikipedia.org/wiki/Random_effects_model) are used when the treatments are not fixed. This occurs when the various factor levels are sampled from a larger population. Because the levels themselves are random variables, some assumptions and the method of contrasting the treatments (a multi-variable generalization of simple differences) differ from the fixed-effects model. They assume that the data being analysed is drawn from a hierarchy of different populations whose differences relate to that hierarchy.\n",
    "- [**Mixed models**](https://en.wikipedia.org/wiki/Mixed_model) contain experimental factors of both fixed and random-effects types, with appropriately different interpretations and analysis for the two types. They are particularly useful in settings where repeated measurements are made on the same statistical units, or where measurements are made on clusters of related statistical units.\n",
    "- **Example:** Teaching experiments could be performed by a college or university department to find a good introductory textbook, with each text considered a treatment. The fixed-effects model would compare a list of candidate texts. The random-effects model would determine whether important differences exist among a list of randomly selected texts. The mixed-effects model would compare the (fixed) incumbent texts to randomly selected alternatives.\n",
    "\n",
    "[**Multi-armed bandit**](http://stevehanov.ca/blog/index.php?id=132): The multi-armed bandit problem models an agent that simultaneously attempts to acquire new knowledge (called \"exploration\") and optimize his or her decisions based on existing knowledge (called \"exploitation\"). The agent attempts to balance these competing tasks in order to maximize his or her total value over the period of time considered.[<sup>1](https://en.wikipedia.org/wiki/Multi-armed_bandit) The name \"multi-armed bandit\" describes a hypothetical experiment where you face several slot machines (\"one-armed bandits\") with potentially different expected payouts. You want to find the slot machine with the best payout rate, but you also want to maximize your winnings. The fundamental tension is between \"exploiting\" arms that have performed well in the past and \"exploring\" new or seemingly inferior arms in case they might perform even better.[<sup>2](https://support.google.com/analytics/answer/2844870?hl=en)\n",
    "\n",
    "**Be careful of [multiple comparisons](#multiple_comparisons).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. How would you run an A/B test if the observations are extremely right-skewed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take samples of your observations and perform statistical analysis on the means of the samples taken. The [Central Limit Theorem](https://www.khanacademy.org/math/statistics-probability/sampling-distributions-library/sample-means/v/central-limit-theorem) states that given certain conditions, the arithmetic mean of a sufficiently large number of iterates of independent random variables, each with a well-defined (finite) expected value and finite variance, will be approximately normally distributed, regardless of the underlying distribution.\n",
    "\n",
    "Note that the Central Limit Theorem is an asymptotic result. Only in the limit of an infinite number of observations will it become exact, but it often provides an excellent approximation even for finite numbers of observations.\n",
    "<img src='images/CLT_dice_example.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. I have two different experiments that both change the sign-up button to my website. I want to test them at the same time. What kinds of things should I keep in mind?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. What is a p-value? What is the difference between type-1 and type-2 error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `p` value is the probability of observing the data we observed, or \"more extreme\", given the null hypothesis is true.\n",
    "\n",
    "A type 1 error, or a \"false positive\", is the incorrect rejection of the null hypothesis. It is asserting something that is absent, a false hit.\n",
    "\n",
    "A type 2 error, or a \"false negative\", is incorrectly retaining a false null hypothesis. It is failing to assert what is present, a miss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. You are AirBnB and you want to test the hypothesis that a greater number of photographs increases the chances that a buyer selects the listing. How would you test this hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. How would you design an experiment to determine the impact of latency on user engagement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 12. What is maximum likelihood estimation? Could there be any case where it doesn’t exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **maximum likelihood estimation** is a method of estimating the parameters of a statistical model given observations, by finding the parameter values that maximize the likelihood of making the observations given the parameters. \n",
    "- For example, one may be interested in the heights of adult female penguins, but be unable to measure the height of every single penguin in a population due to cost or time constraints. Assuming that the heights are normally distributed with some unknown mean and variance, the mean and variance can be estimated with MLE while only knowing the heights of some sample of the overall population. MLE would accomplish this by taking the mean and variance as parameters and finding particular parametric values that make the observed results the most probable given the model.[<sup>1](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) \n",
    "- Note that this does not imply in any way that the temperature measurements are actually drawn from a normal distribution. Instead, it means that we are attempting to find the Gaussian which fits the data in the best fashion. While this distinction may appear subtle, it is critical: we do not assume that our model accurately reflects reality. Instead, we simply try doing the best possible job at modeling the data given a specified model class.[<sup>2](http://alex.smola.org/drafts/thebook.pdf#page=74)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. What's the difference between a MAP, MOM, MLE estimator? In which cases would you want to use each?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. What is a confidence interval and how do you interpret it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. What is unbiasedness as a property of an estimator? Is this always a desirable property when performing inference? What about in data analysis or predictive modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='simpsons_paradox'></a>\n",
    "### [Simpson's Paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox)\n",
    "\n",
    "Simpson's paradox is a paradox in which a trend appears in different groups of data but disappears or reverses when these groups are combined. This result is often encountered in social-science and medical-science statistics, and is particularly confounding when frequency data is unduly given causal interpretations.\n",
    "\n",
    "Main factors that contribute to Simpson's paradox cases:\n",
    "- *Sample sizes.* If the sample sizes of the groups being combined have a high variance, one does not simply add percentages. Because math.\n",
    "- *Confounding variables.* Generally, these cases are looking at one feature influencing the observation. Such as smoking mothers effecting baby birth weights. But other things influence baby birth weights that are much worse than smoking. \n",
    "\n",
    "Duh. \n",
    "\n",
    "Statistics in the media makes me so mad.\n",
    "\n",
    "**I was originally going to include examples here, but I think you should just read all of the ones provided by the Wikipedia article. Simpson's Paradox comes up a lot in the real world and it's very interesting. It might help to see several examples from different business scenarios so you might be able to apply it to your business case.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='multiple_comparisons'></a>\n",
    "### [Multiple Comparisons](https://en.wikipedia.org/wiki/Multiple_comparisons_problem)\n",
    "\n",
    "Multiple comparisons is the phenomenon where an apparently statistically significant observation may have actually arisen by chance because of the size of the parameter space to be searched. In this context the term \"comparisons\" refers to variables or features. \"Multiple comparisons\" arise when a statistical analysis encompasses a number of features, with the presumption that attention will focus on the strongest differences among all features.\n",
    "\n",
    "**Examples:**<br>\n",
    "- Suppose the treatment is a new way of teaching writing to students, and the control is the standard way of teaching writing. Students in the two groups can be compared in terms of grammar, spelling, organization, content, and so on. As more attributes are compared, it becomes more likely that the treatment and control groups will appear to differ on at least one attribute by random chance alone.\n",
    "- Suppose we consider the efficacy of a drug in terms of the reduction of any one of a number of disease symptoms. As more symptoms are considered, it becomes more likely that the drug will appear to be an improvement over existing drugs in terms of at least one symptom.\n",
    "- Suppose we consider the safety of a drug in terms of the occurrences of different types of side effects. As more types of side effects are considered, it becomes more likely that the new drug will appear to be less safe than existing drugs in terms of at least one side effect.\n",
    "\n",
    "In all three examples, as the number of comparisons increases, it becomes more likely that the groups being compared will appear to differ in terms of at least one attribute. Our confidence that a result will generalize to independent data should generally be weaker if it is observed as part of an analysis that involves multiple comparisons, rather than an analysis that involves only a single comparison.\n",
    "\n",
    "Since hypothesis testing is based on rejecting the null hypothesis if the likelihood of the observed data under the null hypotheses is low, and the chance of a rare event increases with multiple comparisons, the likelihood of incorrectly rejecting a null hypothesis (i.e., making a Type I error) increases.\n",
    "\n",
    "**Ways to deal with multiple comparisons:**\n",
    "- [**Bonferroni correction**](https://en.wikipedia.org/wiki/Bonferroni_correction) adjusts confidence intervals by the ratio of significance level to the number of variables.\n",
    "    - Most commonly use correction of multiple comparisons.\n",
    "    - Criticized as being too conservation.\n",
    "- **Resampling**: Bootstrapping (yay!) or Monte Carlo simulations."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
