{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (Given a Dataset) Analyze this dataset and give me a model that can predict this response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kaggle](https://www.kaggle.com/)\n",
    "\n",
    "**Pro Tip**\n",
    "- If asked to predict a response variable during your interview, you should favor simpler models that run quickly and which you can easily explain. If the task is specifically a predictive modeling task, you should try to do, or at least mention, cross-validation as it really is the golden standard to evaluate the quality of one's model. Talk about and justify your approach while you're doing it, and leave some time to plot and visualize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What could be some issues if the distribution of the test data is significantly different than the distribution of the training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue where the distribution of the test data is significantly different than the distribution of the training data is called a *dataset shift*. A few types of dataset shift are as follows:\n",
    "\n",
    "**Covariate shift** is when only the distributions of covariates x change and everything else is the same.\n",
    "- The training and test input follow different distributions, but functional relation remains unchanged.\n",
    "- There are potential computational advantages of\n",
    "adjusting for covariate shift due to the fact that it may be possible to use a simpler\n",
    "model class but only focus on a local region relevant to the test scenario, rather than\n",
    "worrying about the global fit of the model.\n",
    "- A typical example of covariate shift occurs in assessing the risk of future events given current scenarios.\n",
    "- The most common type of dataset shift.\n",
    "<img src='images/covariate_shift.png' width='500'>\n",
    "\n",
    "**Prior probability shift** is when only the distribution over y changes and everything else stays the same.\n",
    "- A popular example stems from the availability of naive Bayes models for the filtering of spam email. If in the test data, the vast majority of the emails contain spammy words, rather than hammy words, we would rate $P(spam) = 0$ as an unlikely model compared with other models such as $P(spam) = 0.7$. In saying this we are implicitly using some a priori model of what distributions $P(spam)$ are acceptable to us, and then using the data to refine this model.\n",
    "<img src='images/prior_probability_shift.png' width='500'>\n",
    "\n",
    "**Sample selection bias** is when the distributions differ as a result of an unknown sample rejection process.\n",
    "- Sample selection bias occurs when the training data points do not accurately represent the distribution of the test scenario due to a selection process for each item that is (usually implicitly) dependent on the target variable.\n",
    "- Common examples are in survey design. A typical street survey, for example, is potentially biased against people with poor mobility who may be more likely to be using other transport methods than walking. A survey in a train station is more likely to catch people engaging in leisure travel than busy commuters with optimized journeys who may refuse to do the survey for lack of time.\n",
    "- Another example would be, in obtaining handwritten characters, completely unintelligible characters may be discarded. But it may be that certain characters are more likely to be written unclearly.\n",
    "<img src='images/selection_bias.png' width='500'>\n",
    "\n",
    "**Imbalanced data** is a form of deliberate dataset shift for computational or modeling convenience.\n",
    "- Imbalanced data is a multiclass machine learning problem where one or more classes are very rare compared with others.\n",
    "- The output predictions need to be weighted by the reciprocal of the known bias and renormalized in order to get the correct predictive probabilities. In theory these renormalized probabilities should be used in the likelihood and hence in any error function being optimized.\n",
    "- Result of sample selection bias where the data corresponding to the common class is discarded, simply because typically that is less valuable: the common class may already be easy to characterize fairly well as it has large amounts of data already. The result of discarding data, though, is that the distribution in the training scenario no longer matches the imbalanced test scenario. However it is this imbalanced scenario that the model will be used for.\n",
    "<img src='images/imbalanced_data.png' width='500'>\n",
    "\n",
    "**Domain shift** involves changes in measurement.\n",
    "- Domain shift is characterized by the fact that the measurement system, or method of description, can change.\n",
    "- A good example of this is estimating gamma correction for photographs. Gamma correction is a specific parametric nonlinear map of pixel intensities. Given two unregistered photographs of a similar scene from different cameras, the appearance may be different due to the camera gamma calibration or due to postprocessing. By optimizing the parameter to best match the pixel distributions we can obtain a gamma correction such that the two photographs are using the same representation.\n",
    "<img src='images/domain_shift.png' width='500'>\n",
    "\n",
    "**Source component shift** involves changes in strength of contributing components.\n",
    "- The observed data is made up from data from a number of different sources, each with its own characteristics, and the proportions of those sources can vary between training and test scenarios.\n",
    "- Three types of source component shift:\n",
    "    - *Mixture component shift*: samples that could come from one of a number of subpopulations, between which the quantity to be predicted may vary\n",
    "        - The data consists directly of samples of values that come from a number of different sources and for each datum the actual source (which we denote by s) is unknown.\n",
    "    - *Factor component shift*: samples chosen are subject to factors that are not fully controlled for, and that could change in different scenarios.\n",
    "        - Factor component shift occurs when the form of the factors remains the same, but the strength of the factors changes between training and test scenario.\n",
    "    - *Mixing component shift*: targets that are aggregate values averaged over a potentially varying population.\n",
    "<img src='images/source_component_shift.png' width='500'>\n",
    "\n",
    "\n",
    "**Reference:**<br>\n",
    "[Dataset Shift in Machine Learning](http://www.acad.bg/ebook/ml/The.MIT.Press.Dataset.Shift.in.Machine.Learning.Feb.2009.eBook-DDU.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What are some ways I can make my model more robust to outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some changes you can make to your model:\n",
    "- **Use a model that's resistant to outliers.** Tree-based models are generally not as affected by outliers, while regression-based models are. If you're performing a statistical test, try a non-parametric test instead of a parametric one.\n",
    "- **Use a more robust error metric.** Switching from mean squared error to mean absolute difference (or something like [Huber Loss](https://en.wikipedia.org/wiki/Huber_loss)) reduces the influence of outliers.\n",
    "\n",
    "Some changes you can make to your data:\n",
    "- **[Winsorize](https://en.wikipedia.org/wiki/Winsorizing) your data.** Artificially cap your data at some threshold.\n",
    "- **Transform your data.** If your data has a very pronounced right tail, try a log transformation.\n",
    "- **Preprocess your data.** Remove outliers, create dummy variables, etc.\n",
    "\n",
    "**Reference:**<br>\n",
    "[Quora thread](https://www.quora.com/What-are-methods-to-make-a-predictive-model-more-robust-to-outliers#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What are some differences you would expect in a model that minimizes squared error, versus a model that minimizes absolute error? In which cases would each error metric be appropriate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean absolute error (MAE) assigns equal weight to the data whereas mean square error (MSE) emphasizes the extremes - the square of a very small number (smaller than 1) is even smaller, and the square of a big number is even bigger, making MAE more robust to outliers than MSE. However, mean squared error is easier to solve for because the derivatives are continuous.\n",
    "\n",
    "MSE is more useful when large errors are particularly undesirable since it gives relatively large weights to large errors.\n",
    "\n",
    "**Note:** The MSE will always be larger or equal to the MAE; the greater difference between them, the greater the variance in the individual errors in the sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. What error metric would you use to evaluate how good a binary classifier is? What if the classes are imbalanced? What if there are more than 2 groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate how good a binary classifier is, the following error metrics are useful in different scenarios:\n",
    "\n",
    "**Accuracy **: the proportion of instances you predict correctly, or the proximity of measurement results to the true value.\n",
    "- Accuracy = $ \\frac{TP + TN}{TP + FP + TN + FN} $\n",
    "- The common first metric to look at because it is very intuitive and easy to explain.\n",
    "- However, it works poorly when the signal in the data is weak compared to the signal from the class imbalance. Also, you cannot express your uncertainty about a certain prediction.\n",
    "<img src='images/accuracy_precision.png' width='250'>\n",
    "\n",
    "**Precision**: also known as the *positive predictive value*, is the repeatability, or reproducibility of the measurement.\n",
    "- Precision = $ \\frac{TP}{TP + FP} $\n",
    "- We want to use precision as a metric when a Type I error (false positive) is less favorable. The higher the precision, the better.\n",
    "\n",
    "**Recall**: also known as the *sensitivity* or *true positive rate*, \n",
    "- Recall = $ \\frac{TP}{TP + FN} $\n",
    "- We want to use recall as a metric when a Type II error (false negative) is less favorable. The higher the recall, the better.\n",
    "<img src='images/precision_recall.png' width='300'>\n",
    "\n",
    "**F1 score**: is the harmonic mean of precision and recall.\n",
    "- $ F_1 = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall} $\n",
    "- Often used in information retrieval for measuring search and classification performance. Also used in natural language processing.\n",
    "\n",
    "**Receiver operating curve (ROC)**: illustrates the performance of a binary classifier system as its discrimination threshold is varied.\n",
    "- False positive rate (x-axis) plotted against the true positive rate (y-axis), which depicts relative trade-offs between true positive (benefits) and false positive (costs).\n",
    "- Easier to explain than precision-recall.\n",
    "- However, if true negatives are not meaningful to the problem or if negative answers dampen the positive effect, precision-recall will be more effective.[<sup>1</sup>](https://www.kaggle.com/forums/f/15/kaggle-forum/t/7517/precision-recall-auc-vs-roc-auc-for-class-imbalance-problems/41179#post41179)\n",
    "\n",
    "    ```\n",
    "    For illustration, let's take an example of an information retrieval problem \n",
    "    where we want to find a set of, say, 100 relevant documents out of a list of \n",
    "    1 million possibilities based on some query. Let's say we've got two \n",
    "    algorithms we want to compare with the following performance:\n",
    "\n",
    "    Method 1: 100 retrieved documents, 90 relevant\n",
    "    Method 2: 2000 retrieved documents, 90 relevant\n",
    "    Clearly, Method 1's result is preferable since they both come back with the \n",
    "    same number of relevant results, but Method 2 brings a ton of false positives \n",
    "    with it. The ROC measures of TPR and FPR will reflect that, but since the \n",
    "    number of irrelevant documents dwarfs the number of relevant ones, the \n",
    "    difference is mostly lost:\n",
    "\n",
    "    Method 1: 0.9 TPR, 0.00001 FPR\n",
    "    Method 2: 0.9 TPR, 0.00191 FPR (difference of 0.0019)\n",
    "    Precision and recall, however, don't consider true negatives and thus won't \n",
    "    be affected by the relative imbalance (which is precisely why they're used \n",
    "    for these types of problems):\n",
    "\n",
    "    Method 1: 0.9 recall, 0.9 precision\n",
    "    Method 2: 0.9 recall, 0.045 precision (difference of 0.855)\n",
    "    ```\n",
    "\n",
    "**Loss function for classification**[<sup>2](https://www.quora.com/What-is-an-intuitive-explanation-for-the-log-loss-function): \n",
    "- Fits a log linear probability model to a set of binary labeled examples.\n",
    "- Measures the uncertainty of the probabilities of your model by comparing them to the true labels. \n",
    "\n",
    "For **class imbalances**, avoid using accuracy as an error metric and favor either precision-recall AUC or ROC AUC according to the previously stated reasons instead.\n",
    "\n",
    "For **multi-class classification**, accuracy can be used as an error metric, or use a more robust error metric like balanced accuracy.<br>\n",
    "Balanced Accuracy = $ \\frac{sensitivity + specificity}{2} $, where *sensitivity* (true positive rate) = $ \\frac{TP}{TP + FN} $ and *specificity* (true negative rate) = $ \\frac{TN}{TN + FP} $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. What are various ways to predict a binary response variable? Can you compare two of them and tell me when one would be more appropriate? What's the difference between these? (SVM, Logistic Regression, Naive Bayes, Decision Tree, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**<br>\n",
    "Generally want to start with logistic regression because:\n",
    "- It's a well-behaved classification algorithm that can be trained as long as you expect your features to be roughly linear and the problem to be linearly separable.\n",
    "- Pretty robust to noise and overfitting can be avoided by using L<sub>2</sub> or L<sub>1</sub> regularization.\n",
    "- Unlike decision trees or SVMs, and you can easily update your model to take in new data.\n",
    "- Performs well with few features or categories.\n",
    "- Computationally easy to train.\n",
    "\n",
    "Logistic regression is prone to underfitting and may have low accuracy.\n",
    "\n",
    "Not really relevant to this question, but it's good to note that the output for a logistic regression can be interpreted as probabilistic which can be useful for ranking rather than classifications for multi-class problems.\n",
    "\n",
    "**Support Vector Machines**\n",
    "- If your data is not linearly separable, use SVMs instead of logistic regression.\n",
    "- Better for higher-dimensional spaces, such as text data.\n",
    "- Works well with small datasets.\n",
    "\n",
    "**Tree Ensembles**\n",
    "- Generally want to try this after logistic regression.\n",
    "- One main advantage is that they do not expect linear features or even features that interact linearly.\n",
    "- Handles binary classifications very well and requires very little data preparation.\n",
    "- Handles high-dimensional spaces and large amounts of training examples very well.\n",
    "- Gradient boosted trees generally perform better than random trees, but are more prone to overfitting.\n",
    "- Robust to outliers.\n",
    "\n",
    "**Naive Bayes**\n",
    "- Needs very little training data.\n",
    "- Easy to build and can be modified with new training data without rebuilding the model.\n",
    "\n",
    "Remember that naive Bayes is naive. In most cases, features are not independent, so naive Bayes may not be an accurate assumption.\n",
    "\n",
    "**Deep Learning**\n",
    "- Generally used for image classification as it takes a lot of data and features.\n",
    "- Computationally slow and expensive to train.\n",
    "\n",
    "<img src='images/ml_algorithm_flowchart.png' width='700'>\n",
    "**Reference:**<br>\n",
    "[Quora Thread](https://www.quora.com/What-are-the-advantages-of-different-classification-algorithms) - Read this thread. Most of my answers are from here, but you should read the more in-depth answers.<br>\n",
    "[The best explanation of SVMs ever](https://www.quora.com/What-are-Kernels-in-Machine-Learning-and-SVM/answer/Rahul-Agarwal-10?srid=fkun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. What is regularization and where might it be helpful? What is an example of using regularization in a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is a technique used to prevent overfitting. It sets a level of complexity to your algorithm to prevent overfitting the data with a parameter $ \\lambda $. $ \\lambda $ is set by cross-validating your training data with test data to find the best $ \\lambda $ to fit both test and training data.\n",
    "\n",
    "Regularization is helpful whenever you have an algorithm that is overfitting the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Why might it be preferable to include fewer predictors over many?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Often leads to overfitting.** Reducing the number of features included in your algorithm or using regularization are good ways to avoid overfitting.\n",
    "- **Easier to interpret.** Including fewer predictors may lead to easier interpretability and explanation to MBAs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Given training data on tweets and their retweets, how would you predict the number of retweets of a given tweet after 7 days after only observing 2 days worth of data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. How could you collect and analyze data to use social media to predict the weather?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. How would you construct a feed to show relevant content for a site that involves user interactions with items?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. How would you design the people you may know feature on LinkedIn or Facebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. How would you predict who someone may want to send a Snapchat or Gmail to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. How would you suggest to a franchise where to open a new store?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. In a search engine, given partial data on what the user has typed, how would you predict the user’s eventual search query?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Given a database of all previous alumni donations to your university, how would you predict which recent alumni are most likely to donate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. You're Uber and you want to design a heatmap to recommend to drivers where to wait for a passenger. How would you approach this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. How would you build a model to predict a March Madness bracket?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. You want to run a regression to predict the probability of a flight delay, but there are flights with delays of up to 12 hours that are really messing up your model. How can you address this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
